> I noticed following during training with saved dataset:
> - policy loss is in range of -4 to 2, it appears a little bit unstable, but won't go too far
> - value loss is very stable, it is around 0.1 to 0.25
> - human loss (imitation learning loss) is around 0.2 to 0.5, I previously thought it would be larger, so I 
> rescaled it with a factor of 0.1 to 0.5, but this reduces the loss to 0.02 to 0.25, making it even more 
> uncompetitive to policy loss. In other words, the imitation learning loss is too small so the algorithm is not 
> learning from human demonstration, it becomes merely pure reinforcement learning.

> My solution is to multiply the imitation learning loss with a factor of 3, so that it is more competitive to
> policy loss. Imitation learning loss is not in range 0.3 to 1.2.

> If the result after this change is good, then this change proves that combination of RL and IL is better than pure 
> RL in this project.