# This config is for PracticeFieldEnv
# This practice field is a long rectangle road, with many objects on road sides
# the road is not wide, the vehicle can easily collide even with human driver
# This practice field is to train the agent to move in narrow space and avoid many objects
practice_fields:
    - [[186.22, 60.68], [188.96, 54.22], [255.87, 54.17], [255.16, 60.68]]

# Client and world configuration
host: localhost
port: 2000
world: Town01
traffic_manager_port: 8000
traffic_manager_seed: 0
timeout: 300.0
vehicle_model: model3
point_sparsity: 20

# Lidar sensor configuration
lidar_height: 3
noise_stddev: "0.1"
channels: "32"
range: "20"
rotation_frequency: "20"
lidar_fov_lower: "-39"
lidar_fov_upper: "-5"
lidar_pps: "56000"  # points per second, 12000 = 6 channels * 100 points per channel * 20 Hz


# Reward config
collide_penalty: -1     # base penalty on collision
point_reach_reward: 2   # fixed reward when reaching a waypoint

# Training configuration
route_path: null                # This path can be set to route.pkl to use the pre-defined route
model_path: "Checkpoints/20230807-045453/model_it15000.pth"
batch_size: 64
learning_rate: 0.0001
gamma: 0.99
memory_size: 5000               # maximum size of memory list
memory_save_freq: 5000          # frequency of saving memory list to disk
cache_size: 10                  # env process will keep this much memory records maximumly, records in cache will pass to train process and stored in memory list
epsilon: 0.03                   # 0.03 - 20 corresponds to around 0.35 epsilon normally
greedy_rand_action_prob: 0.5    # If not random action, the probability of choosing the greedy action from greedy agent
n_repeat_rand_actions: 20       # number of times a random action is applied
target_model_update_freq: 5000  # frequency of updating target model in A2C algorithm
env_model_update_freq: 100      # frequency of copying the modle in train process to env process
moving_average_window: 100      # the window size for compute moving average of reward and loss
print_freq: 100                 # print loss, model output and target action information frequency (training iterations)
show_freq: 50                   # display policy space with the input state frequency (training iterations)
log_freq: 500                   # log frequency (training iterations)
log_dir: "Logs"                 # where to save log messages
save_freq: 1000                 # saving frequency (training iterations)
save_dir: "Checkpoints"         # where to save the model


# display
display: True
show_waypoints: True    # whether to draw waypoints on the simulator monitor window
show_lidar: False       # whether to draw lidar points
show_lidar_map: True    # whether to show lidar map, the current state


# Other
reach_threshold: 2.0    # how close from the ego vehicle to waypoint is considered reached
device: "cuda"          # device, cuda or cpu
n_other_actors: 50      # number of other vehicles and pedestrians on the simulator map
